#!/usr/bin/env python3"""Utilidades para manejo de archivos - Progol Engine v2Funciones para validación, procesamiento y gestión de archivos"""import pandas as pdimport numpy as npimport jsonimport loggingfrom pathlib import Pathimport shutilimport zipfileimport ioimport hashlibfrom datetime import datetime, timedeltafrom typing import Dict, List, Tuple, Optional, Union, Anyimport csvimport reimport fitz  # PyMuPDF para PDFsfrom dataclasses import dataclass# Configuración de logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)@dataclassclass FileValidationResult:    """Resultado de validación de archivo"""    is_valid: bool    file_type: str    file_size: int    row_count: Optional[int] = None    column_count: Optional[int] = None    errors: List[str] = None    warnings: List[str] = None    metadata: Dict[str, Any] = Noneclass FileValidator:    """Validador de archivos para Progol Engine"""        def __init__(self):        self.max_file_size_mb = 200        self.required_columns = {            'progol': ['jornada', 'partido', 'local', 'visitante'],            'odds': ['partido', 'local', 'empate', 'visitante'],            'elo': ['equipo', 'elo_rating'],            'squad': ['equipo', 'valor_mercado']        }                self.expected_data_types = {            'progol': {                'jornada': 'int',                'partido': 'int',                'goles_local': 'int',                'goles_visitante': 'int'            },            'odds': {                'partido': 'int',                'local': 'float',                'empate': 'float',                 'visitante': 'float'            }        }        def validate_file(self, file_path: Union[str, Path], file_type: str) -> FileValidationResult:        """Validar archivo según su tipo"""        file_path = Path(file_path)                # Validaciones básicas        if not file_path.exists():            return FileValidationResult(                is_valid=False,                file_type=file_type,                file_size=0,                errors=["Archivo no encontrado"]            )                file_size = file_path.stat().st_size                if file_size > self.max_file_size_mb * 1024 * 1024:            return FileValidationResult(                is_valid=False,                file_type=file_type,                file_size=file_size,                errors=[f"Archivo demasiado grande: {file_size/(1024*1024):.1f}MB > {self.max_file_size_mb}MB"]            )                # Validación específica por tipo        if file_type == 'progol':            return self._validate_progol_csv(file_path)        elif file_type == 'odds':            return self._validate_odds_csv(file_path)        elif file_type == 'elo':            return self._validate_elo_csv(file_path)        elif file_type == 'squad':            return self._validate_squad_csv(file_path)        elif file_type == 'previas':            return self._validate_previas_pdf(file_path)        else:            return self._validate_generic_csv(file_path, file_type)        def _validate_progol_csv(self, file_path: Path) -> FileValidationResult:        """Validar archivo Progol.csv"""        try:            df = pd.read_csv(file_path)            errors = []            warnings = []                        # Verificar columnas requeridas            required = self.required_columns['progol']            missing_cols = [col for col in required if col not in df.columns]            if missing_cols:                errors.append(f"Columnas faltantes: {missing_cols}")                        # Verificar número de partidos            if len(df) < 14:                errors.append(f"Muy pocos partidos: {len(df)} < 14 mínimo")            elif len(df) > 21:                warnings.append(f"Muchos partidos: {len(df)} > 21 máximo recomendado")                        # Verificar tipos de datos            if 'partido' in df.columns:                if not df['partido'].dtype in ['int64', 'int32']:                    try:                        df['partido'] = pd.to_numeric(df['partido'], errors='coerce')                        if df['partido'].isna().any():                            errors.append("Valores no numéricos en columna 'partido'")                    except:                        errors.append("No se puede convertir 'partido' a numérico")                        # Verificar rango de partidos            if 'partido' in df.columns and not errors:                partido_min, partido_max = df['partido'].min(), df['partido'].max()                if partido_min != 1:                    errors.append(f"Primer partido debe ser 1, encontrado: {partido_min}")                if partido_max != len(df):                    warnings.append(f"Numeración de partidos inconsistente: máx={partido_max}, total={len(df)}")                        # Verificar jornadas            if 'jornada' in df.columns:                jornadas_unicas = df['jornada'].unique()                if len(jornadas_unicas) > 1:                    warnings.append(f"Múltiples jornadas encontradas: {jornadas_unicas}")                        # Metadatos            metadata = {                'jornadas': list(df['jornada'].unique()) if 'jornada' in df.columns else [],                'partidos_regulares': len(df[df['partido'] <= 14]) if 'partido' in df.columns else 0,                'partidos_revancha': len(df[df['partido'] > 14]) if 'partido' in df.columns else 0,                'equipos_locales': list(df['local'].unique()) if 'local' in df.columns else [],                'equipos_visitantes': list(df['visitante'].unique()) if 'visitante' in df.columns else []            }                        return FileValidationResult(                is_valid=len(errors) == 0,                file_type='progol',                file_size=file_path.stat().st_size,                row_count=len(df),                column_count=len(df.columns),                errors=errors,                warnings=warnings,                metadata=metadata            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type='progol',                file_size=file_path.stat().st_size,                errors=[f"Error leyendo CSV: {str(e)}"]            )        def _validate_odds_csv(self, file_path: Path) -> FileValidationResult:        """Validar archivo odds.csv"""        try:            df = pd.read_csv(file_path)            errors = []            warnings = []                        # Verificar columnas básicas            required_cols = ['partido']            odds_cols = ['local', 'empate', 'visitante']                        missing_required = [col for col in required_cols if col not in df.columns]            if missing_required:                errors.append(f"Columnas requeridas faltantes: {missing_required}")                        # Buscar columnas de odds (pueden tener nombres variados)            found_odds_cols = []            for col in df.columns:                col_lower = col.lower()                if any(keyword in col_lower for keyword in ['local', 'home', 'casa']):                    found_odds_cols.append(('local', col))                elif any(keyword in col_lower for keyword in ['empate', 'draw', 'tie']):                    found_odds_cols.append(('empate', col))                elif any(keyword in col_lower for keyword in ['visitante', 'away', 'visit']):                    found_odds_cols.append(('visitante', col))                        if len(found_odds_cols) < 3:                errors.append(f"No se encontraron las 3 columnas de odds. Encontradas: {[col[1] for col in found_odds_cols]}")                        # Validar valores de odds            for odds_type, col_name in found_odds_cols:                if col_name in df.columns:                    # Verificar que sean numéricos                    try:                        numeric_col = pd.to_numeric(df[col_name], errors='coerce')                        na_count = numeric_col.isna().sum()                        if na_count > 0:                            warnings.append(f"Valores no numéricos en {col_name}: {na_count} casos")                                                # Verificar rango de odds (deben ser > 1.0)                        valid_odds = numeric_col.dropna()                        if len(valid_odds) > 0:                            min_odd, max_odd = valid_odds.min(), valid_odds.max()                            if min_odd < 1.0:                                warnings.append(f"Odds muy bajos en {col_name}: mín={min_odd:.2f}")                            if max_odd > 50.0:                                warnings.append(f"Odds muy altos en {col_name}: máx={max_odd:.2f}")                    except:                        errors.append(f"Error procesando columna {col_name}")                        # Metadatos            metadata = {                'columnas_odds_encontradas': [col[1] for col in found_odds_cols],                'partidos_con_odds': len(df),                'rango_partidos': [df['partido'].min(), df['partido'].max()] if 'partido' in df.columns else None            }                        return FileValidationResult(                is_valid=len(errors) == 0,                file_type='odds',                file_size=file_path.stat().st_size,                row_count=len(df),                column_count=len(df.columns),                errors=errors,                warnings=warnings,                metadata=metadata            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type='odds',                file_size=file_path.stat().st_size,                errors=[f"Error leyendo CSV: {str(e)}"]            )        def _validate_elo_csv(self, file_path: Path) -> FileValidationResult:        """Validar archivo elo.csv"""        try:            df = pd.read_csv(file_path)            errors = []            warnings = []                        required = ['equipo']            missing = [col for col in required if col not in df.columns]            if missing:                errors.append(f"Columnas faltantes: {missing}")                        # Buscar columna de rating ELO            elo_col = None            for col in df.columns:                if any(keyword in col.lower() for keyword in ['elo', 'rating', 'rank']):                    elo_col = col                    break                        if elo_col is None:                errors.append("No se encontró columna de rating ELO")            else:                # Validar ratings                try:                    ratings = pd.to_numeric(df[elo_col], errors='coerce')                    na_count = ratings.isna().sum()                    if na_count > 0:                        warnings.append(f"Ratings no numéricos: {na_count} casos")                                        valid_ratings = ratings.dropna()                    if len(valid_ratings) > 0:                        min_rating, max_rating = valid_ratings.min(), valid_ratings.max()                        if min_rating < 800 or max_rating > 2800:                            warnings.append(f"Ratings fuera del rango típico: {min_rating:.0f}-{max_rating:.0f}")                except:                    errors.append(f"Error procesando ratings en {elo_col}")                        metadata = {                'columna_elo': elo_col,                'equipos_unicos': len(df['equipo'].unique()) if 'equipo' in df.columns else 0,                'rating_promedio': float(pd.to_numeric(df[elo_col], errors='coerce').mean()) if elo_col else None            }                        return FileValidationResult(                is_valid=len(errors) == 0,                file_type='elo',                file_size=file_path.stat().st_size,                row_count=len(df),                column_count=len(df.columns),                errors=errors,                warnings=warnings,                metadata=metadata            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type='elo',                file_size=file_path.stat().st_size,                errors=[f"Error leyendo CSV: {str(e)}"]            )        def _validate_squad_csv(self, file_path: Path) -> FileValidationResult:        """Validar archivo squad_value.csv"""        try:            df = pd.read_csv(file_path)            errors = []            warnings = []                        required = ['equipo']            missing = [col for col in required if col not in df.columns]            if missing:                errors.append(f"Columnas faltantes: {missing}")                        # Buscar columna de valor            value_col = None            for col in df.columns:                if any(keyword in col.lower() for keyword in ['valor', 'value', 'precio', 'mercado']):                    value_col = col                    break                        if value_col is None:                errors.append("No se encontró columna de valor de mercado")            else:                # Validar valores                try:                    values = pd.to_numeric(df[value_col], errors='coerce')                    na_count = values.isna().sum()                    if na_count > 0:                        warnings.append(f"Valores no numéricos: {na_count} casos")                                        valid_values = values.dropna()                    if len(valid_values) > 0:                        min_val, max_val = valid_values.min(), valid_values.max()                        if min_val < 0:                            errors.append("Valores negativos encontrados")                        if max_val > 1e9:  # > 1 billón                            warnings.append(f"Valores muy altos: máx={max_val:,.0f}")                except:                    errors.append(f"Error procesando valores en {value_col}")                        metadata = {                'columna_valor': value_col,                'equipos_unicos': len(df['equipo'].unique()) if 'equipo' in df.columns else 0,                'valor_promedio': float(pd.to_numeric(df[value_col], errors='coerce').mean()) if value_col else None            }                        return FileValidationResult(                is_valid=len(errors) == 0,                file_type='squad',                file_size=file_path.stat().st_size,                row_count=len(df),                column_count=len(df.columns),                errors=errors,                warnings=warnings,                metadata=metadata            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type='squad',                file_size=file_path.stat().st_size,                errors=[f"Error leyendo CSV: {str(e)}"]            )        def _validate_previas_pdf(self, file_path: Path) -> FileValidationResult:        """Validar archivo previas.pdf"""        try:            doc = fitz.open(file_path)            errors = []            warnings = []                        # Verificar que se puede abrir            if doc.page_count == 0:                errors.append("PDF vacío o corrupto")                        # Extraer texto de muestra            text_sample = ""            if doc.page_count > 0:                page = doc[0]                text_sample = page.get_text()[:500]  # Primeros 500 caracteres                        doc.close()                        # Verificar contenido relevante            keywords = ['progol', 'quiniela', 'partido', 'local', 'visitante', 'forma', 'lesion']            found_keywords = [kw for kw in keywords if kw.lower() in text_sample.lower()]                        if len(found_keywords) < 2:                warnings.append(f"Pocas palabras clave relevantes encontradas: {found_keywords}")                        metadata = {                'paginas': doc.page_count if 'doc' in locals() else 0,                'texto_muestra': text_sample[:200],                'keywords_encontradas': found_keywords            }                        return FileValidationResult(                is_valid=len(errors) == 0,                file_type='previas',                file_size=file_path.stat().st_size,                errors=errors,                warnings=warnings,                metadata=metadata            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type='previas',                file_size=file_path.stat().st_size,                errors=[f"Error leyendo PDF: {str(e)}"]            )        def _validate_generic_csv(self, file_path: Path, file_type: str) -> FileValidationResult:        """Validar CSV genérico"""        try:            df = pd.read_csv(file_path)                        return FileValidationResult(                is_valid=True,                file_type=file_type,                file_size=file_path.stat().st_size,                row_count=len(df),                column_count=len(df.columns),                metadata={'columnas': list(df.columns)}            )                    except Exception as e:            return FileValidationResult(                is_valid=False,                file_type=file_type,                file_size=file_path.stat().st_size,                errors=[f"Error leyendo CSV: {str(e)}"]            )class FileProcessor:    """Procesador de archivos para pipeline"""        def __init__(self, output_dir: str = "data/processed"):        self.output_dir = Path(output_dir)        self.output_dir.mkdir(parents=True, exist_ok=True)        def process_progol_file(self, file_path: Path, jornada: str) -> str:        """Procesar archivo Progol.csv"""        df = pd.read_csv(file_path)                # Estandarizar columnas        df = self._standardize_progol_columns(df)                # Agregar información de tipo de partido        df['tipo'] = df['partido'].apply(lambda x: 'Regular' if x <= 14 else 'Revancha')                # Agregar match_id        df['match_id'] = df.apply(lambda row: f"{jornada}-{row['partido']}", axis=1)                # Guardar archivo procesado        output_path = self.output_dir / f"matches_{jornada}.csv"        df.to_csv(output_path, index=False)                logger.info(f"✅ Progol procesado: {len(df)} partidos → {output_path}")        return str(output_path)        def process_odds_file(self, file_path: Path, jornada: str) -> str:        """Procesar archivo odds.csv"""        df = pd.read_csv(file_path)                # Estandarizar columnas de odds        df = self._standardize_odds_columns(df)                # Calcular probabilidades implícitas        for col in ['local', 'empate', 'visitante']:            if col in df.columns:                df[f'prob_raw_{col}'] = 1 / df[col]                # Normalizar probabilidades (eliminar vig)        prob_cols = [col for col in df.columns if col.startswith('prob_raw_')]        if len(prob_cols) == 3:            df['prob_sum'] = df[prob_cols].sum(axis=1)            for col in prob_cols:                normalized_col = col.replace('prob_raw_', 'prob_norm_')                df[normalized_col] = df[col] / df['prob_sum']                # Agregar match_id        df['match_id'] = df.apply(lambda row: f"{jornada}-{row['partido']}", axis=1)                # Guardar archivo procesado        output_path = self.output_dir / f"odds_processed_{jornada}.csv"        df.to_csv(output_path, index=False)                logger.info(f"✅ Odds procesados: {len(df)} partidos → {output_path}")        return str(output_path)        def _standardize_progol_columns(self, df: pd.DataFrame) -> pd.DataFrame:        """Estandarizar nombres de columnas de Progol"""        column_mapping = {            'Jornada': 'jornada',            'Partido': 'partido',            'Local': 'local',            'Visitante': 'visitante',            'GolesLocal': 'goles_local',            'GolesVisitante': 'goles_visitante',            'Resultado': 'resultado'        }                # Mapeo flexible (case-insensitive)        for col in df.columns:            for original, standard in column_mapping.items():                if col.lower() == original.lower():                    df = df.rename(columns={col: standard})                    break                return df        def _standardize_odds_columns(self, df: pd.DataFrame) -> pd.DataFrame:        """Estandarizar nombres de columnas de odds"""        # Mapeo de posibles nombres de columnas        mappings = {            'local': ['local', 'home', 'casa', '1'],            'empate': ['empate', 'draw', 'tie', 'x'],            'visitante': ['visitante', 'away', 'visit', '2']        }                for standard_name, possible_names in mappings.items():            for col in df.columns:                if any(name.lower() in col.lower() for name in possible_names):                    if standard_name not in df.columns:  # Solo si no existe ya                        df = df.rename(columns={col: standard_name})                        break                return dfclass FileManager:    """Gestor de archivos del sistema"""        def __init__(self, base_dir: str = "data"):        self.base_dir = Path(base_dir)        self.raw_dir = self.base_dir / "raw"        self.processed_dir = self.base_dir / "processed"        self.outputs_dir = self.base_dir / "outputs"        self.dashboard_dir = self.base_dir / "dashboard"                # Crear directorios        for directory in [self.raw_dir, self.processed_dir, self.outputs_dir, self.dashboard_dir]:            directory.mkdir(parents=True, exist_ok=True)        def get_file_info(self, directory: str = "processed") -> List[Dict]:        """Obtener información de archivos en directorio"""        target_dir = getattr(self, f"{directory}_dir")        files_info = []                for file_path in target_dir.glob("*"):            if file_path.is_file():                stat = file_path.stat()                files_info.append({                    'name': file_path.name,                    'path': str(file_path),                    'size': stat.st_size,                    'size_mb': stat.st_size / (1024 * 1024),                    'modified': datetime.fromtimestamp(stat.st_mtime),                    'extension': file_path.suffix,                    'type': self._get_file_type(file_path.name)                })                return sorted(files_info, key=lambda x: x['modified'], reverse=True)        def _get_file_type(self, filename: str) -> str:        """Determinar tipo de archivo por nombre"""        filename_lower = filename.lower()                if 'portfolio' in filename_lower or 'portafolio' in filename_lower:            return 'Portfolio'        elif 'prob' in filename_lower:            return 'Probabilidades'        elif 'simulation' in filename_lower or 'sim' in filename_lower:            return 'Simulación'        elif 'matches' in filename_lower or 'partidos' in filename_lower:            return 'Partidos'        elif 'odds' in filename_lower:            return 'Odds'        elif 'stats' in filename_lower:            return 'Estadísticas'        else:            return 'Otro'        def create_backup(self, jornada: str) -> str:        """Crear backup de archivos de jornada"""        backup_dir = self.base_dir / "backups" / f"backup_{jornada}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"        backup_dir.mkdir(parents=True, exist_ok=True)                # Backup de processed        processed_files = list(self.processed_dir.glob(f"*{jornada}*"))        for file_path in processed_files:            shutil.copy2(file_path, backup_dir)                # Backup de outputs        output_files = list(self.outputs_dir.glob(f"*{jornada}*"))        for file_path in output_files:            shutil.copy2(file_path, backup_dir)                logger.info(f"Backup creado: {backup_dir} ({len(processed_files + output_files)} archivos)")        return str(backup_dir)        def clean_old_files(self, days_old: int = 30) -> List[str]:        """Limpiar archivos antiguos"""        cutoff_date = datetime.now() - timedelta(days=days_old)        deleted_files = []                for directory in [self.processed_dir, self.outputs_dir]:            for file_path in directory.glob("*"):                if file_path.is_file():                    file_modified = datetime.fromtimestamp(file_path.stat().st_mtime)                    if file_modified < cutoff_date:                        file_path.unlink()                        deleted_files.append(str(file_path))                logger.info(f"Archivos eliminados: {len(deleted_files)} (> {days_old} días)")        return deleted_files        def create_zip_archive(self, jornada: str, include_raw: bool = False) -> bytes:        """Crear archivo ZIP con archivos de jornada"""        zip_buffer = io.BytesIO()                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:            # Archivos procesados            for file_path in self.processed_dir.glob(f"*{jornada}*"):                zip_file.write(file_path, f"processed/{file_path.name}")                        # Archivos de salida            for file_path in self.outputs_dir.glob(f"*{jornada}*"):                zip_file.write(file_path, f"outputs/{file_path.name}")                        # Archivos raw (opcional)            if include_raw:                for file_path in self.raw_dir.glob("*"):                    zip_file.write(file_path, f"raw/{file_path.name}")                zip_buffer.seek(0)        return zip_buffer.getvalue()        def get_file_hash(self, file_path: Union[str, Path]) -> str:        """Calcular hash MD5 de archivo"""        hash_md5 = hashlib.md5()        with open(file_path, "rb") as f:            for chunk in iter(lambda: f.read(4096), b""):                hash_md5.update(chunk)        return hash_md5.hexdigest()        def copy_to_dashboard(self, jornada: str) -> List[str]:        """Copiar archivos relevantes al directorio dashboard"""        copied_files = []                # Archivos importantes para dashboard        important_files = [            f"portfolio_final_{jornada}.csv",            f"simulation_metrics_{jornada}.csv",            f"prob_final_{jornada}.csv",            f"portfolio_stats_{jornada}.json"        ]                for filename in important_files:            source_path = self.processed_dir / filename            if source_path.exists():                dest_path = self.dashboard_dir / filename                shutil.copy2(source_path, dest_path)                copied_files.append(filename)                logger.info(f"Archivos copiados a dashboard: {len(copied_files)}")        return copied_filesclass TemplateGenerator:    """Generador de templates de archivos"""        def __init__(self):        self.templates = {            'progol': self._generate_progol_template,            'odds': self._generate_odds_template,            'elo': self._generate_elo_template,            'squad': self._generate_squad_template        }        def generate_template(self, template_type: str, n_partidos: int = 21,                          include_sample_data: bool = True) -> pd.DataFrame:        """Generar template de archivo"""        if template_type not in self.templates:            raise ValueError(f"Template no soportado: {template_type}")                return self.templates[template_type](n_partidos, include_sample_data)        def _generate_progol_template(self, n_partidos: int, include_sample: bool) -> pd.DataFrame:        """Generar template de Progol.csv"""        data = []                for i in range(n_partidos):            partido_num = i + 1            tipo = "Regular" if partido_num <= 14 else "Revancha"                        if include_sample:                row = {                    'jornada': 2283,                    'partido': partido_num,                    'tipo': tipo,                    'local': f'Equipo_Local_{partido_num}',                    'visitante': f'Equipo_Visitante_{partido_num}',                    'goles_local': np.random.randint(0, 4) if include_sample else '',                    'goles_visitante': np.random.randint(0, 4) if include_sample else '',                    'resultado': ''                }            else:                row = {                    'jornada': '',                    'partido': partido_num,                    'tipo': tipo,                    'local': '',                    'visitante': '',                    'goles_local': '',                    'goles_visitante': '',                    'resultado': ''                }                        data.append(row)                return pd.DataFrame(data)        def _generate_odds_template(self, n_partidos: int, include_sample: bool) -> pd.DataFrame:        """Generar template de odds.csv"""        data = []                for i in range(n_partidos):            partido_num = i + 1                        if include_sample:                # Generar odds realistas                prob_l = np.random.uniform(0.25, 0.55)                prob_e = np.random.uniform(0.20, 0.35)                prob_v = 1.0 - prob_l - prob_e                                # Convertir a odds con margen de casa                margin = 1.05  # 5% margen                odd_l = margin / prob_l                odd_e = margin / prob_e                odd_v = margin / prob_v                                row = {                    'partido': partido_num,                    'local': round(odd_l, 2),                    'empate': round(odd_e, 2),                    'visitante': round(odd_v, 2),                    'casa_apuestas': 'Bet365',                    'fecha_actualizacion': datetime.now().strftime('%Y-%m-%d %H:%M')                }            else:                row = {                    'partido': partido_num,                    'local': '',                    'empate': '',                    'visitante': '',                    'casa_apuestas': '',                    'fecha_actualizacion': ''                }                        data.append(row)                return pd.DataFrame(data)        def _generate_elo_template(self, n_partidos: int, include_sample: bool) -> pd.DataFrame:        """Generar template de elo.csv"""        # Generar lista de equipos únicos        equipos = set()        for i in range(n_partidos):            equipos.add(f'Equipo_Local_{i+1}')            equipos.add(f'Equipo_Visitante_{i+1}')                data = []        for equipo in sorted(equipos):            if include_sample:                row = {                    'equipo': equipo,                    'elo_rating': np.random.randint(1200, 1800),                    'fecha_actualizacion': datetime.now().strftime('%Y-%m-%d'),                    'partidos_jugados': np.random.randint(20, 50),                    'victorias': np.random.randint(5, 25),                    'empates': np.random.randint(5, 15),                    'derrotas': np.random.randint(5, 20)                }            else:                row = {                    'equipo': equipo if include_sample else '',                    'elo_rating': '',                    'fecha_actualizacion': '',                    'partidos_jugados': '',                    'victorias': '',                    'empates': '',                    'derrotas': ''                }                        data.append(row)                return pd.DataFrame(data)        def _generate_squad_template(self, n_partidos: int, include_sample: bool) -> pd.DataFrame:        """Generar template de squad_value.csv"""        # Generar lista de equipos únicos        equipos = set()        for i in range(n_partidos):            equipos.add(f'Equipo_Local_{i+1}')            equipos.add(f'Equipo_Visitante_{i+1}')                data = []        for equipo in sorted(equipos):            if include_sample:                row = {                    'equipo': equipo,                    'valor_mercado': np.random.randint(50_000_000, 500_000_000),  # 50M - 500M                    'moneda': 'EUR',                    'fecha_actualizacion': datetime.now().strftime('%Y-%m-%d'),                    'fuente': 'Transfermarkt',                    'jugadores_plantilla': np.random.randint(25, 35),                    'edad_promedio': round(np.random.uniform(23.5, 28.5), 1)                }            else:                row = {                    'equipo': equipo if include_sample else '',                    'valor_mercado': '',                    'moneda': '',                    'fecha_actualizacion': '',                    'fuente': '',                    'jugadores_plantilla': '',                    'edad_promedio': ''                }                        data.append(row)                return pd.DataFrame(data)        def save_template(self, template_type: str, n_partidos: int = 21,                      output_dir: str = "data/templates",                      include_sample_data: bool = False) -> str:        """Guardar template a archivo"""        output_path = Path(output_dir)        output_path.mkdir(parents=True, exist_ok=True)                df_template = self.generate_template(template_type, n_partidos, include_sample_data)                sample_suffix = "_con_datos" if include_sample_data else "_vacio"        filename = f"{template_type}_template_{n_partidos}partidos{sample_suffix}.csv"        file_path = output_path / filename                df_template.to_csv(file_path, index=False)        logger.info(f"Template guardado: {file_path}")                return str(file_path)# Funciones de utilidaddef get_file_hash(file_path: Union[str, Path]) -> str:    """Calcular hash MD5 de archivo"""    return FileManager().get_file_hash(file_path)def validate_all_files(jornada: str) -> Dict[str, FileValidationResult]:    """Validar todos los archivos de una jornada"""    validator = FileValidator()    manager = FileManager()        results = {}        # Archivos a validar    files_to_check = [        ('progol', manager.raw_dir / "Progol.csv"),        ('odds', manager.raw_dir / "odds.csv"),        ('elo', manager.raw_dir / "elo.csv"),        ('squad', manager.raw_dir / "squad_value.csv"),        ('previas', manager.raw_dir / "previas.pdf")    ]        for file_type, file_path in files_to_check:        if file_path.exists():            results[file_type] = validator.validate_file(file_path, file_type)        else:            results[file_type] = FileValidationResult(                is_valid=False,                file_type=file_type,                file_size=0,                errors=[f"Archivo no encontrado: {file_path}"]            )        return resultsdef create_file_report(jornada: str) -> Dict[str, Any]:    """Crear reporte completo de archivos"""    manager = FileManager()        report = {        'jornada': jornada,        'timestamp': datetime.now().isoformat(),        'directories': {            'raw': manager.get_file_info('raw'),            'processed': manager.get_file_info('processed'),            'outputs': manager.get_file_info('outputs'),            'dashboard': manager.get_file_info('dashboard')        },        'validation': validate_all_files(jornada),        'summary': {            'total_files': 0,            'total_size_mb': 0,            'valid_files': 0,            'invalid_files': 0        }    }        # Calcular resumen    for dir_files in report['directories'].values():        report['summary']['total_files'] += len(dir_files)        report['summary']['total_size_mb'] += sum(f['size_mb'] for f in dir_files)        for validation in report['validation'].values():        if validation.is_valid:            report['summary']['valid_files'] += 1        else:            report['summary']['invalid_files'] += 1        return report# Exports principales__all__ = [    'FileValidator', 'FileProcessor', 'FileManager', 'TemplateGenerator',    'FileValidationResult', 'validate_all_files', 'create_file_report',    'get_file_hash']